{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***Decision Tree***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2JQDTmkYL0l0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "**Answer:**\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm that splits data into branches based on feature values, forming a tree-like structure.\n",
        "\n",
        "At each node, it chooses the best feature to split the data.\n",
        "\n",
        "Internal nodes represent features, branches represent decisions, and leaves represent outcomes (class labels).\n",
        "\n",
        "In classification, the algorithm predicts the class label by following the path from root to leaf."
      ],
      "metadata": {
        "id": "xpQb-CvaL0on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "Gini Impurity: Measures how often a randomly chosen element would be misclassified.\n",
        "Formula:\n",
        "G\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "p\n",
        "i\n",
        "2\n",
        "G=1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "\n",
        "Entropy: Measures disorder or uncertainty in the dataset.\n",
        "Formula:\n",
        "H\n",
        "=\n",
        "−\n",
        "∑\n",
        "p\n",
        "i\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "p\n",
        "i\n",
        ")\n",
        "H=−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "\n",
        "Impact:\n",
        "\n",
        "Lower Gini/Entropy means purer nodes.\n",
        "\n",
        "The Decision Tree splits features to minimize impurity, making each node as pure as possible"
      ],
      "metadata": {
        "id": "kQfD0XcML0rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "**Pre-Pruning:** Stop tree growth early using conditions (e.g., max_depth, min_samples_split).\n",
        "Advantage: Prevents overfitting and reduces training time.\n",
        "\n",
        "**Post-Pruning:** Build full tree, then cut back unnecessary branches.\n",
        "Advantage: Allows more flexibility and often yields better generalization."
      ],
      "metadata": {
        "id": "nKKNnvUEL0vR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Information Gain (IG): Reduction in impurity (Entropy or Gini) after splitting.\n",
        "\n",
        "Formula:\n",
        "\n",
        "I\n",
        "G\n",
        "=\n",
        "H\n",
        "(\n",
        "p\n",
        "a\n",
        "r\n",
        "e\n",
        "n\n",
        "t\n",
        ")\n",
        "−\n",
        "∑\n",
        "n\n",
        "i\n",
        "n\n",
        "H\n",
        "(\n",
        "c\n",
        "h\n",
        "i\n",
        "l\n",
        "d\n",
        "i\n",
        ")\n",
        "IG=H(parent)−∑\n",
        "n\n",
        "n\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " H(child\n",
        "i\n",
        "​\n",
        " )\n",
        "Importance: Helps select the best feature for splitting, ensuring meaningful decision boundaries."
      ],
      "metadata": {
        "id": "pcJ89k1kL0yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "**Answer:**\n",
        "\n",
        "Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Credit risk analysis\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Marketing/customer segmentation\n",
        "\n",
        "Advantages: Easy to interpret, handles both categorical & numerical data, non-parametric.\n",
        "\n",
        "Limitations: Can overfit easily, unstable to small changes, less accurate compared to ensemble methods"
      ],
      "metadata": {
        "id": "yzmT9hhXL00c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#question 5\n",
        "# Question 8\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Output results\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK4FxFvuQi5_",
        "outputId": "b23f9604-77c5-465a-b3c0-117f20c2eaf7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#06Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI14fldrMagd",
        "outputId": "55c1af18-73e0-41a9-e907-9a09bf8c86ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "#Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        " #fully-grown tree.\n",
        " # Fully-grown tree\n",
        "# Question 7\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X,y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fully-grown tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Limited depth tree\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "print(\"Accuracy of fully-grown tree:\", acc_full)\n",
        "print(\"Accuracy of depth-limited tree (max_depth=3):\", acc_limited)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J85fjVPkMel0",
        "outputId": "6c27b91b-477f-4eb9-aa49-5805b821deab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 1.0\n",
            "Accuracy of depth-limited tree (max_depth=3): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qwjZSifMked",
        "outputId": "f093c056-9edf-4a29-c900-f05dea00083c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "#GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target  # discrete class labels (0, 1, 2)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(dt, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Make predictions using the best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2YtO3DAMrxu",
        "outputId": "f12524b4-7ab0-48bb-dd3a-96e4ee64025f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "#wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "#mixed data types and some missing values.\n",
        "#Explain the step-by-step process you would follow to:\n",
        "#● Handle the missing values\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters\n",
        "#● Evaluate its performance\n",
        "#And describe what business value this model could provide in the real-world\n",
        "#setting.\n",
        "\n",
        "**Answer:**\n",
        "**Handle Missing Values:**\n",
        "\n",
        "Use mean/median for numerical features.\n",
        "\n",
        "Use mode or most frequent category for categorical features.\n",
        "\n",
        "**Advanced:** Use imputation methods (KNN, MICE).\n",
        "\n",
        "**Encode Categorical Features:**\n",
        "\n",
        "One-Hot Encoding for nominal categories.\n",
        "\n",
        "Label Encoding for ordinal categories.\n",
        "\n",
        "Train Decision Tree Model:\n",
        "\n",
        "Split data into train/test.\n",
        "\n",
        "Fit DecisionTreeClassifier.\n",
        "\n",
        "**Tune Hyperparameters:**\n",
        "\n",
        "Use GridSearchCV/RandomizedSearchCV for max_depth, min_samples_split, criterion.\n",
        "\n",
        "**Evaluate Performance:**\n",
        "\n",
        "Use Accuracy, Precision, Recall, F1-score, ROC-AUC.\n",
        "\n",
        "**Business Value:**\n",
        "\n",
        "Helps doctors quickly identify high-risk patients.\n",
        "\n",
        "Reduces healthcare costs by prioritizing urgent cases.\n",
        "\n",
        "Improves patient outcomes through early disease prediction."
      ],
      "metadata": {
        "id": "kF6DXM92S0I0"
      }
    }
  ]
}